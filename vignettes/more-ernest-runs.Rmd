---
title: "More Nested Sampling Runs with Ernest"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{More Nested Sampling Runs with Ernest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include = FALSE, eval = FALSE}
#' @srrstats {G1.5} These examples are based on classical test problems for
#' nested sampling used within the nestle package. These problems are
#' also used within ernest's test suite to validate its behaviour.
#' @srrstats {G5.1} Tests and test data are provided here for users to
#' interact with.
#' @srrstats {BS1.1} Demonstrates how to enter data through an
#' anonymous function.
```

```{r setup}
library(ernest)
set.seed(42)
```

ernest uses comprehensive tests to verify its sampling behaviour against the Python package [nestle](https://github.com/kbarbary/nestle/blob/master/). This vignette describes some of these tests and demonstrates how to construct likelihood functions for ernest.

## Two Gaussian "Blobs"

We estimate the Bayesian evidence for a simple two-dimensional problem with two well-separated Gaussian modes—a classical nested sampling test problem, as the evidence can be solved analytically.

Define the log-likelihood and prior:

```{r}
# Log-likelihood for two Gaussian blobs
gaussian_blobs_loglik <- function(x) {
  sigma <- 0.1
  mu1 <- c(1, 1)
  mu2 <- -c(1, 1)
  sigma_inv <- diag(2) / sigma^2

  if (!is.matrix(x)) dim(x) <- c(1, length(x))
  dx1 <- sweep(x, 2, mu1)
  dx2 <- sweep(x, 2, mu2)
  val1 <- -0.5 * rowSums((dx1 %*% sigma_inv) * dx1)
  val2 <- -0.5 * rowSums((dx2 %*% sigma_inv) * dx2)
  matrixStats::rowLogSumExps(cbind(val1, val2))
}

# Uniform prior over [-5, 5] in each dimension
prior <- create_uniform_prior(lower = -5, upper = 5, names = c("A", "B"))
```

Set up the sampler and run nested sampling:

```{r}
sampler <- ernest_sampler(gaussian_blobs_loglik, prior, n_points = 100)
result <- generate(sampler, show_progress = FALSE)
```

For this distribution, the analytical evidence is $\mathcal{Z} = \log(2^2 * \pi * 0.1^2 / 100) \approx -6.679$. Use `summary` to extract the log-evidence (`log_evidence`) and its uncertainty (`log_evidence_err`):

```{r}
smry <- summary(result)
smry
```

Plot the progress of the evidence estimate:

```{r}
plot(result)
```

## Example: Estimating the Evidence for the Eggbox Problem

The "eggbox" problem is a challenging test case for nested sampling due to its highly multimodal likelihood surface. Define the log-likelihood and prior:

```{r}
eggbox_loglik <- function(x) {
  tmax <- 5.0 * pi
  if (!is.matrix(x)) dim(x) <- c(1, length(x))
  t <- sweep(2.0 * tmax * x, 2, tmax, "-")
  (2.0 + cos(t[, 1] / 2.0) * cos(t[, 2] / 2.0))^5.0
}

# Uniform prior over [0, 1] in each dimension
eggbox_prior <- create_uniform_prior(names = c("A", "B"))
```

Visualize the likelihood surface:

```{r echo = FALSE}
eggbox_sample <- expand.grid(
  "A" = seq(0, 1, by = 0.01),
  "B" = seq(0, 1, by = 0.01)
)
eggbox_sample$logl <- mapply(
  function(a, b) eggbox_loglik(c(a, b)),
  eggbox_sample$A,
  eggbox_sample$B
)

library(ggplot2)
ggplot(eggbox_sample, aes(A, B, fill = logl)) +
  geom_tile() +
  scale_fill_viridis_c("Log-Lik.")
```

Run the sampler and compare the estimated log-evidence to the nestle result ($\approx 235.895$):

```{r}
sampler <- ernest_sampler(eggbox_loglik, eggbox_prior)
result <- generate(sampler, show_progress = FALSE)
smry <- summary(result)
smry
```

Plot the posterior distribution:

```{r}
visualize(result, type = "density")
```

## Adding Data

Often, the likelihood depends on observed data. In ernest, you must incorporate the data within your likelihood function. Here, we show how to supply data using both `create_likelihood()` and an anonymous function, with a certified dataset from the U.S. National Institute of Science and Technology ([NIST](https://www.itl.nist.gov/div898/strd/mcmc/mcmc01.html)).

```{r}
# Sample data: vector of observations
y <- c(
  100000000.2, 100000000.1, 100000000.3, 100000000.1, 100000000.3,
  100000000.1, 100000000.3, 100000000.1, 100000000.3, 100000000.1, 100000000.3
)

# Model likelihood: Y ~ N(mu, sd)
log_lik <- function(theta, data) {
  if (theta[2] <= 0) {
    return(-Inf)
  }
  sum(stats::dnorm(y, mean = theta[1], sd = theta[2], log = TRUE))
}
```

Note: `log_lik` returns `-Inf` for non-positive standard deviations, explicitly censoring impossible parameter values.

Supply data using either an anonymous function or the dots argument of `create_likelihood`:

```{r}
# Anonymous function
anon_log_lik <- \(theta) log_lik(theta, data = y)

# Using dots for create_likelihood
expected_mean <- 100000000.200000000000000
expected_sd <- 0.108372230793914
anon_log_lik(c(expected_mean, expected_sd))
```

Define the prior for the mean and standard deviation, then run the sampler. The prior is constrained near the expected posterior values: the number of iterations required is proportional to the product of the number of live points (`n_points`) and the [KL divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) between prior and posterior.

```{r}
prior <- create_uniform_prior(
  lower = c(99999999, 0.01),
  upper = c(100000001, 1),
  names = c("mu", "sigma")
)
sampler <- ernest_sampler(anon_log_lik, prior)
result <- generate(sampler, show_progress = FALSE)
```

Examine the estimated posterior distribution:

```{r}
draws <- as_draws(result) |>
  posterior::resample_draws()
```

Summarize the posterior for each parameter (e.g., median and 95% interval):

```{r}
posterior::summarise_draws(
  draws,
  \(x) quantile(x, probs = c(0.05, 0.5, 0.95))
)
```
