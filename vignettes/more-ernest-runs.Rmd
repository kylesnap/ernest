---
title: "More Nested Sampling Runs with ernest"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{More Nested Sampling Runs with ernest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include = FALSE, eval = FALSE}
#' @srrstats {G1.5} These examples are based on classical test problems for
#' nested sampling used within the nestle package. These problems are
#' also used within ernest's test suite to validate its behaviour.
#' @srrstats {G5.1} Tests and test data are provided here for users to
#' interact with.
#' @srrstats {BS1.1} Demonstrates how to enter data through an
#' anonymous function.
```

```{r setup}
library(ernest)
library(posterior)
set.seed(42)
```

ernest uses comprehensive tests to verify its sampling behaviour against the Python package [nestle](https://github.com/kbarbary/nestle/blob/master/). This vignette describes some of these tests and demonstrates how to construct likelihood functions for ernest.

## Two Gaussian "Blobs"

We estimate the Bayesian evidence for a simple two-dimensional problem with two well-separated Gaussian modesâ€”a classical nested sampling test problem, as the evidence can be solved analytically.

Define the log-likelihood and prior:

```{r}
# Log-likelihood for two Gaussian blobs
gaussian_blobs_loglik <- function(x) {
  sigma <- 0.1
  mu1 <- c(1, 1)
  mu2 <- -c(1, 1)
  sigma_inv <- diag(2) / 0.1**2

  dx1 <- -0.5 * mahalanobis(x, c(1, 1), sigma_inv, inverted = TRUE)
  dx2 <- -0.5 * mahalanobis(x, c(-1, -1), sigma_inv, inverted = TRUE)
  matrixStats::colLogSumExps(rbind(dx1, dx2))
}

# Uniform prior over [-5, 5] in each dimension
prior <- create_uniform_prior(lower = -5, upper = 5, names = c("A", "B"))

# Expected log-evidence:
expect_evid <- -6.679
```

Set up the sampler and run nested sampling:

```{r}
sampler <- ernest_sampler(
  create_likelihood(vectorized_fn = gaussian_blobs_loglik),
  prior,
  nlive = 100
)
result <- generate(sampler, show_progress = FALSE)
```

Use `summary` to extract the log-evidence and its uncertainty:

```{r}
summary(result)
```

Plot the progress of the evidence estimate:

```{r}
plot(result)
```

## Example: Estimating the Evidence for the Eggbox Problem

The "eggbox" problem is a challenging test case for nested sampling due to its highly multimodal likelihood surface. Define the log-likelihood and prior:

```{r}
eggbox_loglik <- function(x) {
  tmax <- 5.0 * pi
  if (!is.matrix(x)) dim(x) <- c(1, length(x))
  t <- sweep(2.0 * tmax * x, 2, tmax, "-")
  (2.0 + cos(t[, 1] / 2.0) * cos(t[, 2] / 2.0))^5.0
}

# Uniform prior over [0, 1] in each dimension
eggbox_prior <- create_uniform_prior(names = c("A", "B"))
```

Visualize the likelihood surface:

```{r echo = FALSE}
eggbox_sample <- expand.grid(
  "A" = seq(0, 1, by = 0.01),
  "B" = seq(0, 1, by = 0.01)
)
eggbox_sample$logl <- mapply(
  function(a, b) eggbox_loglik(c(a, b)),
  eggbox_sample$A,
  eggbox_sample$B
)

library(ggplot2)
ggplot(eggbox_sample, aes(A, B, fill = logl)) +
  geom_tile() +
  scale_fill_viridis_c("Log-Lik.")
```

Run the sampler and compare the estimated log-evidence to the nestle result ($\approx 235.895$):

```{r}
sampler <- ernest_sampler(eggbox_loglik, eggbox_prior)
result <- generate(sampler, show_progress = FALSE)
smry <- summary(result)
smry
```

Plot the posterior distribution:

```{r}
visualize(result, .which = "trace")
```

## Adding Data

Often, the likelihood depends on observed data. In ernest, you must incorporate the data within your likelihood function. Here, we show an example of binding data to a log-likelihood function through a function factory. The data is a test case, provided by the U.S. National Institute of Science and Technology ([NIST](https://www.itl.nist.gov/div898/strd/mcmc/mcmc01.html)).

```{r}
# Sample data: vector of observations
y <- c(
  100000000.2, 100000000.1, 100000000.3, 100000000.1, 100000000.3,
  100000000.1, 100000000.3, 100000000.1, 100000000.3, 100000000.1, 100000000.3
)

# Model likelihood: Y ~ N(mu, sd)
gaussian_log_lik <- function(data) {
  force(data)
  
  function(theta) {
    if (theta[2] <= 0) return(-Inf)
    sum(stats::dnorm(data, mean = theta[1], sd = theta[2], log = TRUE))
  }
}

log_lik <- gaussian_log_lik(y)
expected_mean <- 100000000.200000000000000
expected_sd <- 0.108372230793914
log_lik(c(expected_mean, expected_sd))
```
Note how `log_lik` returns `-Inf` for non-positive standard deviations, explicitly censoring impossible parameter values.

Define the prior for the mean and standard deviation, then run the sampler. The prior is constrained so that nested sampling can more quickly begin integrating across the model's posterior:

```{r}
prior <- create_uniform_prior(
  lower = c(99999999, 0.01),
  upper = c(100000001, 1),
  names = c("mu", "sigma")
)
sampler <- ernest_sampler(log_lik, prior)
result <- generate(sampler, show_progress = FALSE)
```

Examine the estimated posterior distribution and prepare a summary:

```{r}
as_draws(result) |>
  resample_draws() |>
  summarise_draws(\(x) quantile(x, probs = c(0.05, 0.5, 0.95)))
```
