---
title: "Quickstart: Nested Sampling with Ernest"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quickstart: Nested Sampling with Ernest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r srr-tags, eval = FALSE, echo = FALSE}
#' roxygen_block_name
#' 
#' @srrstats {G1.3} Vignette presents consistent statistical terminology to be used throughout ernest.
#' @srrstats {BS1.2, BS1.2b} Contains examples for specifying priors.
#' @srrstats {BS1.3, BS1.3a} Describes how to save and continue previous runs with `generate()`.
```

```{r setup, message = FALSE}
library(ernest)
library(ggplot2)
library(posterior)
library(truncnorm)
library(LaplacesDemon)
```

# Nested Sampling with Ernest

This vignette provides a hands-on introduction to nested sampling using the `ernest` package. You will learn how to:

- Understand the role of model evidence in Bayesian inference and why it is difficult to compute.
- Use prior transforms to define parameter spaces for nested sampling.
- Set up and run a nested sampling analysis in R with ernest.
- Inspect, summarise, and visualise results, including evidence and posterior samples.

## Bayesian Model Evidence and Nested Sampling

In Bayesian inference, we use probabilities to represent our current beliefs about a modelâ€™s unobservable parameters. When we gather new data, we update these beliefs using Bayes' theorem:
$$
P(\theta) = \frac{L(\theta)\pi(\theta)}{Z}
$$
where $\pi(\theta)$ is the _prior distribution_, $L(\theta)$ is the _likelihood_ of a model given parameters $\theta$ and the data $D$, and $P(\theta)$ is the _posterior distribution_ of the parameters after our beliefs have been updated.

The denominator $Z$ is called the _Bayesian evidence_ or _marginal likelihood_. In isolation, $Z$ serves to normalise $P(\theta)$ so that it is a well-conditioned probability distribution. If we reorganise Bayes' theorem to isolate $Z$, we see that calculating a model's evidence involves integrating over all possible values of $\theta$:
$$
Z = \int_{\forall \theta} L(\theta) \pi(\theta) d\theta
$$
This allows us to use $Z$ as a parameter-independent measure of a model's overall plausibility given some data. When comparing two models, the ratio of their respective evidences (called the [Bayes factor](https://en.wikipedia.org/wiki/Bayes_factor)) shows how much more the data support one model over the other, which forms the foundation of Bayesian model selection.

For most data and models, the evidence integral cannot be solved directly. Instead, researchers rely on estimation methods. Nested sampling (NS), introduced by @skilling2006, is designed to estimate $Z$ even when the posterior distribution is poorly conditioned (e.g., if $L(\theta)$ has multiple peaks or discontinuities along values of $\theta$). It accomplishes this by dividing the prior space $\pi(\theta)$ into many small nested shells or volumes. These shells are defined by the smallest likelihood value they contain, such that the volume of the cells containing the smallest value $L^*$ is given by
$$
V(L^*) = \int_{L(\theta) > L^*} \pi(\theta) d\theta
$$
If we build many $V(L^*)$ across different values of $L^*$, we can approximate the original multidimensional integral across the parameter space of $\theta$ with a one-dimensional integral over the sequence of $V(L^*)$:
$$
Z = \int_0^1 V^{-1}(L^*) dV
$$
where $V^{-1}(V(L^*)) = L^*$ exists. This requires us to estimate the volume of each shell $V$, which we can do [@speagle2020]. 

NS generates this sequence of shells by generating a set number of live points within the prior space, then replacing the worst of these points with a new point from $\pi(\theta)$ with an additional likelihood constraint. This lets NS handle complicated likelihood surfaces, including those with multiple peaks or sharp transitions. In addition, NS naturally provides stopping rules, meaning the algorithm knows when it has gathered enough information to accurately estimate the evidence. As a bonus, the same samples used for evidence estimation can be repurposed to estimate the posterior distribution.

## Nested Sampling with ernest

Here, we use an example from the [dynesty](https://dynesty.readthedocs.io/en/v2.1.5/crashcourse.html) documentation to demonstrate how to use ernest to design, perform, and report nested sampling runs.

### Defining Priors

Nested sampling operates by drawing samples from the prior, but for efficiency, ernest represents the prior space as points in a [0, 1)-unit hypercube. A _prior transformation_ function must be specified to translate points from the hypercube into valid $\theta$.

In ernest, you define priors using functions like `create_uniform_prior()` or by supplying a custom transformation to `create_prior()`. In addition to organising the prior into an object that ernest can use during NS, these functions also perform checks to help ensure your prior transformation function is size- and type-stable.

#### Example: Uniform Prior

In many cases, it is sufficient to define a prior with independently distributed uniform or normal distributions. Ernest provides convenience functions to build such priors with efficient prior transformation functions.

The following defines a uniform prior over \([-10, 10)\) for each parameter in 3D space:
```{r}
prior <- create_uniform_prior(
  3,
  lower = -10,
  upper = 10,
  varnames = c("x", "y", "z")
)
prior
```

#### Example: Custom/Conditional Prior

For more complex priors, you must provide a custom function. In the case of prior spaces with independent marginals, this amounts to specifying a function that applies the inverse CDF for each component of $\theta$.

Consider the following prior space with five dimensions: The first two are drawn from a bivariate Normal distribution, the third is drawn from a Beta distribution, the fourth from a Gamma distribution, and the fifth from a truncated normal distribution.
```{r}
five_dim <- function(u) {
  x <- double(5)
  # MVN(mu = c(5, 2), Sigma = [5, 4; 4, 5])
  t <- qnorm(u[1:2])
  sigma_sqrt <- matrix(c(2, 1, 1, 2), nrow = 2, byrow = TRUE)
  mu <- c(5, 2)
  x[1:2] <- drop(t %*% sigma_sqrt) + c(5, 2)
  
  # Beta
  x[3] <- qbeta(u[3], shape1 = 2.31, shape2 = 0.627)
  
  # Gamma
  x[4] <- qgamma(u[4], shape = 5)
  
  # Truncated Normal
  x[5] <- qtruncnorm(u[5], a = 2, b = 10, mean = 5, sd = 2)
  
  return(x)
}
create_prior(
  fn = five_dim,
  n_dim = 5,
  varnames = c("MVN", "MVN", "Beta", "Gamma", "Norm[2, 10]")
)
```
For more sophisticated priors (such as those for hierarchical models), you will need to build more involved prior transformation functions.

```{r}
hierarchical <- function(u) {
  # mu ~ N(5, 1)
  mu <- qnorm(u[1], mean = 5, sd = 1)
  # log10(sd) ~ U[-1, 1]
  sd <- 10 ^ qunif(u[2], -1, 1)
  # x ~ N(mu, sd^2)
  x <- qnorm(u[3], mean = mu, sd = sd)
  c(mu, sd, x)
}
create_prior(
  fn = hierarchical,
  n_dim = 3,
  varnames = c("mu", "sigma", "x"),
  lower = c(-Inf, 0, -Inf)
)
```
### Likelihood Function

Model log-likelihoods are represented by R functions. These functions are expected to return a single scalar value for each possible $\theta$ within the prior space. If, for any reason, $\pi(\theta)$ contains regions where $\theta$ is invalid, ensure your likelihood function returns `-Inf`.

In this example, we use `create_likelihood()` to assign parameters to the `LaplaceDemon` density function for a multivariate normal distribution:
```{r}
mu <- c(0, 0, 0)
C <- diag(1, 3)
C[C == 0] <- 0.95

loglike <- create_likelihood(
  rowwise_fn = dmvn,
  mu = !!mu,
  Sigma = !!C,
  log = TRUE
)
```

### Setting Up and Running the Sampler

Initialise the sampler with your likelihood and prior. The number of live points (`n_points`) controls the resolution (more points = higher accuracy, slower run).

```{r}
sampler <- ernest_sampler(
  log_lik = loglike,
  prior = prior,
  n_points = 500
)
sampler
```

Run nested sampling for a fixed number of iterations or until the evidence estimate converges:

```{r}
run <- generate(sampler, max_iterations = 2000, seed = 123)
run
```

`generate` produces an `ernest_run` object that can be saved to disk. You can continue a run by calling `generate` on a previously created `ernest_run`:

```{r}
tmp_name <- tempfile("ernest_run.rds")
saveRDS(run, tmp_name)

continued_run <- readRDS(tmp_name)

run2 <- generate(continued_run, min_logz = 0.01)
run2
```
```{r include = FALSE}
file.remove(tmp_name)
```

### Inspecting and Summarising Results

The result object has a `summary` method for viewing evidence estimates, posterior samples, and diagnostics as a tidy `tibble`:

```{r}
summary(run2)
summary(run2)$run
```

The `posterior` package offers methods for inspecting the points generated during a run. 
```{r}
library(posterior)
unweighted_post <- as_draws(run2)
```

You can view the importance weight of each point and reweight the sample to estimate the posterior distribution:
```{r}
weights(unweighted_post) |> head()
weighted_post <- unweighted_post |>
  resample_draws()
posterior::summarise_draws(weighted_post)
```

### Visualising Results

Ernest provides plotting utilities for evidence, posterior, and diagnostics.

#### Evidence evolution

```{r}
plot(run2)
```

#### Posterior marginals

```{r}
visualize(run2, type = "density")
```

#### Trace plots

```{r}
visualize(run2, type = "trace", vars = c("x", "y", "z"))
```

### Uncertainty and Resampling

You can simulate the uncertainty of an NS run by generating random draws of the log-volume estimate at each iteration (see @skilling2006 for the mathematical rationale of the log-volume estimates). You can further visualise this uncertainty with the `plot` method.

```{r}
calc_sim <- calculate(run2, ndraws = 500)
calc_sim
plot(calc_sim)
```

-----

For more details on nested sampling, please refer to ernest's documentation or the following references:
