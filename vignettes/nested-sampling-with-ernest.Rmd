---
title: "Quickstart: Nested Sampling with Ernest"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quickstart: Nested Sampling with Ernest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r srr-tags, eval = FALSE, echo = FALSE}
#' roxygen_block_name
#' 
#' @srrstats {G1.3} Vignette presents consistent statistical terminology to be
#' used throughout ernest.
#' @srrstats {BS1.2, BS1.2b} Contains examples for specifying priors.
#' @srrstats {BS1.3, BS1.3a} Describes how to save and continue previous runs
#' with `generate()`.
```

```{r setup, message = FALSE}
library(ernest)
library(ggplot2)
library(posterior)
library(truncnorm)
library(LaplacesDemon)
```

# Nested Sampling with Ernest

This vignette provides a quick overview of nested sampling (NS) and the main functions within the ernest package. Most of this information is available scattered throughout the R documentation. After reading this, you should be able to:

-   Identify the role of model evidence within Bayesian data analysis.
-   Understand how and why NS is different from more popular Bayesian computational techniques (i.e., Markov Chain Monte Carlo [MCMC]).
-   Identify the necessary components for a nested sampling run and how to implement them within ernest.
-   Perform, summarise, and visualise the results from an NS run, including the model evidence estimates and posterior samples.

## Bayes' Theory and Model Evidence

In Bayesian inference, we use probabilities to represent our beliefs about a model's unobservable parameters [@ashton2022]. These beliefs can be updated when we evaluate our model $M$ in light of some observed data $D$. Formally, these updates can be expressed using Bayes' theorem:

$$
\Pr(\theta|D,M) = \frac{\Pr(D|\theta, M) \Pr(\theta|M)}{\Pr(D|M)}
$$

Each component of Bayes' theorem has a role:

-   $\Pr(D|\theta,M) = L(\theta)$ is the likelihood function, describing how likely the data is given some parameter estimate,

-   $\Pr(\theta|M) = \pi(\theta)$ is the prior distribution, representing our current knowledge of how a model's parameters are distributed.

-   $\Pr(\theta|D,M) = \Pi(\theta)$ is the posterior distribution, which represents our knowledge of $\theta$'s distribution after it has been updated with $D$, and;

-   $\Pr(D|M) = Z$ is the Bayesian evidence, which is the normalizing constant for $\Pi(\theta)$, allowing the posterior to exist as a well-conditioned probability distribution.

Generally, we have access to our model's likelihood function and prior distribution. The goal of Bayesian calculation is to use these components to estimate a model's posterior distribution and its evidence. Historically, most Bayesian computations have been performed with a focus on estimating the posterior: Packages like [brms](https://cran.r-project.org/package=brms), for example, use techniques derived from MCMC to generate dense chains of samples that approximate $P(\theta)$. In this approach, estimating the evidence usually requires considerable extra work beyond estimating $P(\theta)$.

Because of this extra computational diï¬ƒculty, evidence values are often ignored by researchers or treated as optional by-products @skilling2006. This is unfortunate, because we can use $Z$ as a parameter-independent representation of the probability of generating $D$ under the model $M$. To show this, we can refactore Bayes' theorem and isolate evidence:

$$
Z = \int_{\forall \theta} L(\theta) \pi(\theta) d\theta
$$ Here, we notice that evidence can also be called a model's *marginal likelihood*, calculated by integrating out the prior distribution of the parameters from the likelihood function. Calculating $Z$ allows for us to compare competing models of the same data by using the ratio of their respective evidences (called the Bayes factor). Unlike model selection with maximum likelihood estimates, such comparisons are performed independently from the parameter spaces of each model. Additionally, sharing evidence values for published models can be valuable for future researchers who may wish to build their own competing models.

## Nested Sampling Overview

Nested sampling, introduced in @skilling2004 and @skilling2006, is designed to estimate $Z$ directly, while providing $P(\theta)$ as an optional by-product. These estimates can be generated in scenarios that typically confound MCMC-based sampler, such as when the posterior distribution is poorly conditioned (e.g., if $L(\theta)$ has multiple local maxima).

The goal of NS is to approximate $Z$ by simplifying the complex, multi-dimensional integral over $\pi(\theta)$ into a unidimensional integral over a series of small regions of $\pi(\theta)$. These regions or shells are constructed based on the smallest likelihood value they each contain, such that regions with high likelihoods are nested within regions of small likelihood. For a given likelihood value $L^*$, we can define the volume of prior spacethat contains regions of higher likelihood:$$
V(L^*) = \int_{L(\theta) > L^*} \pi(\theta) d\theta
$$If we take many different values of $L^*$ across the prior and find their respective $V(L^*)$, we can calculate evidence through integrating the likelihood function over these volumes: $$
Z = \int_0^1 V^{-1}(L^*) dV
$$where $V^{-1}(V(L^*)) = L^*$ exists.

In most cases, the likelihood surface of $M$ will make it impossible for us to calculate $V$ directly from $L^*$. Instead, we need to sample values of $L^*$ in a way that allows us to estimate how much $V$ shrinks between iterations. NS accomplishes this through *likelihood-restricted prior sampling* (LRPS): A run starts by randomly generating $N$ live points within the prior space, then at each iteration the point with the lowest likelihood is used to update the evidence estimate as $L^*$, then it is replaced with a point sampled from $\pi(\theta)$ that has greater likelihood than $L^*$. Restricted sampling allows $V$ to shrink at a predictable rate at each iteration $i$ across the run [\@speagle2020]:

$$
\begin{aligned}
  V_0 &= 1 \\
  V_i &= tV_{i-1} \\
  \Pr(t_i) &= Nt_i^{N - 1} \\
\end{aligned}
$$

where $t_i$ is the smallest value within $N$ draws from the standard uniform distribution.

Compared to MCMC-based methods, NS is able to estimate model evidence and quantify its uncertainty without rerunning sampling or without launching multiple chains. NS is also more equipped to handle complicated likelihood surfaces, including those with multiple peaks or sharp transitions. In addition, NS naturally provides a stopping criterion for its runs: Sampling can halt when the remaining unexplored volume of $\pi(\theta)$ stops making significant contributions to our estimate of $Z$. Finally, after the run we can use the likelihood and volume estimates of each sampled point to estimate the posterior distribution.

## Nested Sampling with ernest

The ernest package, like other nested sampling implementations, requires the user to provide the key components of nested sampling: A likelihood function, a prior space specification, and an LRPS method. To demonstrate this in action, we will use an example and data from brms.

The `epilepsy` data describes a longitudinal and randomized trial of an anti-convulsant therapy. Fifty-nine patients were sorted into treatment groups and attended four treatment sessions. Our first goal is to model the number of seizures between each session (`count`) as a function of treatment-control group (`trt`) and the (standardized) number of seizures experienced at baseline (`zBase`). In base-R, this is accomplished through the `glm` function:

```{r}
data(epilepsy)
mod <- glm(count ~ zBase * Trt, family = poisson(), data = epilepsy)
mod$coefficients
```



The prior encodes our current knowledge of how the parameters within our model are distributed. In ernest, the priors are encoded as `ernest_prior` objects. Users create these objects using `create_prior` or one of its specializations (e.g., `create_normal_prior`).

Like other NS implementations, ernest creates and resamples points within a $d$-dimensional unit cube. These points are then transformed to the original parameter space before likelihood values are calculated. In ernest, this *prior transformation function* is provided by the user, and must be able to take a point from the i.i.d. $d$-unit cube and transform it to a $d$-dimensional point within $\pi(\theta)$. For independent parameters, this transformation is identical to the inverse cumulative distribution function (CDF) associated with each parameter; for many distributions within R, this is called the quantile function (e.g., `qnorm`).

`create_prior` allows users to provide their own prior transformation function, along with unique names for each parameter. To demonstrate this, let us assign a $N(0, 10)$ prior to each of the four coefficients within the proposed regression model. In brms, we accomplish this through `r set_prior("normal(0, 10)", class = "b", coef = "x1")`. In ernest, we first construct the transformation function, then wrap and test it with `create_prior`.

```{r}
# Transform each [0,1] value within `x` into the inverse cdf. of N(0, 10)
inverse_t <- function(x) {
  stats::qnorm(x, mean = 0, sd = 10)
}

prior <- create_prior(inverse_t, names = names(mod$coefficients))
prior
```
Alternatively, for simpler priors, we can use the convinience functions `create_normal_prior` or `create_uniform_prior`. We can also combine these priors into a single object using `c`:

```{r}
pooled_prior <- create_normal_prior(
  names = names(mod$coefficients),
  mean = 0,
  sd = 10
)

# Concatenating a N(0, 1) and U[0, 1] prior
unif_p <- create_uniform_prior(names = c("A", "B"))
norm_p <- create_uniform_prior(names = c("C", "D"))
c(unif_p, norm_p)
```

### Likelihood Function
The log-likelihood function should be capable of taking in a (post-transformation) vector of model parameters and returning the corresponding log-likelihood value. If there are regions within $\pi(\theta)$ that contain impossible parameter values, ernest expects that the log-likelihood function will return `-Inf`.

Unlike higher-level R packages for Bayesian inference, ernest does not provide ways to build likelihood functions for specific kinds of models. Users should consult other packages, such as [enrichwith](https://cran.r-project.org/web/packages/enrichwith/) and [tfprobability](https://cran.r-project.org/web/packages/tfprobability/index.html), which are well-equipped to assist users in building their likelihood function. In the case of the `insulin` model, we can quickly build the model's likelihood function using the components from `mod`:

```{r}
new_x <- model.matrix(
  object = mod$formula,
  data = mod$model,
  terms = mod$terms,
  contrasts.arg = attr(model.matrix(mod), "contrasts")
)
new_y <- model.response(mod$model)
linkinv <- stats::poisson()$linkinv

poisson_glm_ll <- function(x) {
  eta <- new_x %*% x
  mu <- linkinv(eta)
  sum(stats::dpois(new_y, lambda = mu, log = TRUE))
}

#' Custom function returns the expected log-lik. at the MLE.
all.equal(
  as.double(stats::logLik(mod)),
  poisson_glm_ll(mod$coefficients),
  check.attributes = FALSE
)
```

The function `create_likelihood` is used to wrap the user's log-likelihood function with several assertions on its output value. It also controls how non-finite likelihood values are encoded if they are provided; by default, these values are replaced with `-Inf` and the user is warned.

### LRPS SECTION?

### Setting Up and Running the Sampler

We can now design a nested sampling run. First, we construct an `ernest_sampler` object: This object is bound to an environment that contains the current set of "live" points and their corresponding likelihood values.

```{r}
sampler <- ernest_sampler(
  log_lik = poisson_glm_ll,
  prior = pooled_prior,
  n_points = 300
)
sampler
```

Nested samples are generated with `generate`. The `min_logz` argument sets the minimum log-ratio between the current (estimated) evidence and the remaining (estimated) evidence in the unexplored prior. Sampling ends once this ratio falls below `min_logz`, at which point its deemed that the remaining unexplored region of $\pi(\theta)$ contains a trivially small fraction of $Z$.

```{r}
run <- generate(sampler, min_logz = 0.1)
run
```

`generate` produces an `ernest_run` object that can be saved. You can continue a run by calling `generate` on a previously created `ernest_run`:

```{r}
tmp_name <- tempfile("ernest_run.rds")
saveRDS(run, tmp_name)

continued_run <- readRDS(tmp_name)

run2 <- generate(continued_run, min_logz = 0.01, show_progress = FALSE)
run2
```

```{r include = FALSE}
file.remove(tmp_name)
```

### Inspecting and Summarising Results

The result object has a `summary` method for viewing evidence estimates, posterior samples, and diagnostics as a tidy `tibble`:

```{r}
summary(run2)
summary(run2)$run
```

The [posterior](https://CRAN.R-project.org/package=posterior) package offers methods for inspecting the points generated during a run. Calling `as_draws` on an `ernest_run` object produces a matrix with posterior log-weights calculated for each point. This matrix be resampled to produce a weighed posterior estimate.

```{r}
library(posterior)
unweighted_post <- as_draws(run2)
weighted_post <- unweighted_post |>
  resample_draws()
posterior::summarise_draws(weighted_post, default_summary_measures())
```

To visualize how the points evolved throughout the run, we use the `visualize` function.

```{r}
# plot(run2)
```

```{r}
visualize(run2, type = "density")
```

```{r}
visualize(run2, type = "trace", vars = c("zBase", "Trt1", "zBase:Trt1"))
```

Finally, we can simulate the uncertainty of an NS run by generating random draws of the log-volume estimate at each iteration [@skilling2006, @higson2019]. This is done using `calculate`.

```{r}
calc_sim <- calculate(run2, ndraws = 500)
calc_sim
```

------------------------------------------------------------------------

For more details on nested sampling, please refer to ernest's documentation.

------------------------------------------------------------------------

\emph{References}
